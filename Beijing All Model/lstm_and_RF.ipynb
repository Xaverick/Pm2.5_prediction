{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Attention, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Load Dataset\n",
    "file_path = \"/kaggle/input/beijing/Beijing.csv\"  # Replace with your file path\n",
    "df = pd.read_csv(file_path)\n",
    "# df = df[:50000]\n",
    "# Explore Dataset\n",
    "print(df.head())\n",
    "\n",
    "# Handle Missing Values\n",
    "df.fillna(method='ffill', inplace=True)  # Forward-fill for missing values\n",
    "df.fillna(method='bfill', inplace=True)  # Backward-fill as a fallback\n",
    "\n",
    "# Encode Categorical Data\n",
    "label_encoder = LabelEncoder()\n",
    "df['wd'] = label_encoder.fit_transform(df['wd'])  # Encode wind direction\n",
    "df['station'] = label_encoder.fit_transform(df['station'])\n",
    "\n",
    "# Feature Engineering\n",
    "df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "# Lag Features (e.g., PM2.5 of previous hours)\n",
    "for lag in [1, 3, 6, 12, 24]:\n",
    "    df[f'PM2.5_lag_{lag}'] = df['PM2.5'].shift(lag)\n",
    "\n",
    "# Rolling Statistics\n",
    "df['PM2.5_roll_mean'] = df['PM2.5'].rolling(window=3).mean()\n",
    "df['PM2.5_roll_std'] = df['PM2.5'].rolling(window=3).std()\n",
    "\n",
    "# Drop Rows with NaNs from Lag Features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Select Features and Target\n",
    "features = ['PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM', 'station'] + \\\n",
    "           [f'PM2.5_lag_{lag}' for lag in [1, 3, 6, 12, 24]] + ['PM2.5_roll_mean', 'PM2.5_roll_std']\n",
    "target = 'PM2.5'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Normalize Data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Reshape for LSTM (Samples, Timesteps, Features)\n",
    "timesteps = 24  # Use last 24 hours for prediction\n",
    "X_lstm = np.array([X_scaled[i - timesteps:i] for i in range(timesteps, len(X_scaled))])\n",
    "y_lstm = y_scaled[timesteps:]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "# LSTM Feature Extraction\n",
    "def build_lstm_feature_model(timesteps, features):\n",
    "    inputs = Input(shape=(timesteps, features))\n",
    "    lstm_out = LSTM(128, return_sequences=False)(inputs)\n",
    "    dense_out = Dense(64, activation='relu')(lstm_out)\n",
    "    outputs = Dense(1, activation='linear')(dense_out)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build and Train LSTM\n",
    "lstm_model = build_lstm_feature_model(timesteps, X_train.shape[2])\n",
    "lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Extract LSTM Features\n",
    "lstm_train_features = lstm_model.predict(X_train)\n",
    "lstm_test_features = lstm_model.predict(X_test)\n",
    "\n",
    "# Combine LSTM Outputs with Original Features\n",
    "X_train_combined = np.hstack([lstm_train_features, X_train[:, -1, :]])  # Last timestep for static features\n",
    "X_test_combined = np.hstack([lstm_test_features, X_test[:, -1, :]])\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train_combined, y_train.ravel())\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = rf.predict(X_test_combined)\n",
    "\n",
    "# Rescale Predictions and Ground Truth\n",
    "y_pred_rescaled = scaler_y.inverse_transform(y_pred.reshape(-1, 1))\n",
    "y_test_rescaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate Performance\n",
    "r2 = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "mse = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## of the above model is Results \n",
    "\n",
    "R² Score: 0.9562 <br>\n",
    "MAE: 9.8764 <br>\n",
    "MSE: 312.4317 <br>\n",
    "RMSE: 17.6757 <br>\n",
    "MAPE: 18.2978"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
