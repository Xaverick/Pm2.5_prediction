{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-17T09:42:32.964366Z",
     "iopub.status.busy": "2024-11-17T09:42:32.963178Z",
     "iopub.status.idle": "2024-11-17T09:42:34.616231Z",
     "shell.execute_reply": "2024-11-17T09:42:34.615001Z",
     "shell.execute_reply.started": "2024-11-17T09:42:32.964287Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/new-data/new_data.csv\n",
      "/kaggle/input/all-other-set/second_set.csv\n",
      "/kaggle/input/all-other-set/third_set.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T10:55:17.866412Z",
     "iopub.status.busy": "2024-11-17T10:55:17.865692Z",
     "iopub.status.idle": "2024-11-17T10:57:41.253904Z",
     "shell.execute_reply": "2024-11-17T10:57:41.252443Z",
     "shell.execute_reply.started": "2024-11-17T10:55:17.866323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sizes: 159804 119304 73050\n",
      "Combined balanced size: 352158\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def preprocess_air_quality_data(file_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df.drop(columns=['StationId'], inplace=True)\n",
    "    \n",
    "    # Handle missing values\n",
    "    numeric_columns = df.select_dtypes(include=['float64']).columns\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "    # Process datetime and create time features\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'], format='mixed', errors='coerce')\n",
    "    df['Year'] = df['Datetime'].dt.year\n",
    "    df['Month'] = df['Datetime'].dt.month\n",
    "    df['Day'] = df['Datetime'].dt.day\n",
    "    df['Hour'] = df['Datetime'].dt.hour\n",
    "    \n",
    "    # Categorize PM2.5 values\n",
    "    bins = [0, 12, 35.4, 55.4, 150.4, 250.4, float('inf')]\n",
    "    labels = ['Good', 'Moderate', 'Unhealthy for Sensitive', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n",
    "    df['PM2.5_Category'] = pd.cut(df['PM2.5'], bins=bins, labels=labels)\n",
    "    \n",
    "    # Perform undersampling\n",
    "    min_class_size = df['PM2.5_Category'].value_counts().min()\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for category in df['PM2.5_Category'].unique():\n",
    "        category_df = df[df['PM2.5_Category'] == category]\n",
    "        if len(category_df) > min_class_size:\n",
    "            balanced_dfs.append(category_df.sample(min_class_size))\n",
    "        else:\n",
    "            balanced_dfs.append(category_df)\n",
    "    \n",
    "    df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    return df_balanced\n",
    "\n",
    "\n",
    "# For each dataset\n",
    "df1 = preprocess_air_quality_data('/kaggle/input/new-data/new_data.csv')\n",
    "df2 = preprocess_air_quality_data('/kaggle/input/all-other-set/second_set.csv')\n",
    "df3 = preprocess_air_quality_data('/kaggle/input/all-other-set/third_set.csv')\n",
    "\n",
    "# Combine all datasets\n",
    "final_df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "# Print sizes to see the reduction\n",
    "print(\"Original sizes:\", len(df1), len(df2), len(df3))\n",
    "print(\"Combined balanced size:\", len(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T11:00:37.735564Z",
     "iopub.status.busy": "2024-11-17T11:00:37.733452Z",
     "iopub.status.idle": "2024-11-17T11:00:47.096065Z",
     "shell.execute_reply": "2024-11-17T11:00:47.093985Z",
     "shell.execute_reply.started": "2024-11-17T11:00:37.735493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('/kaggle/working/balanced_air_quality_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# def preprocess_air_quality_data(file_path, num_samples=50000, sequence_length=100):\n",
    "#     \"\"\"\n",
    "#     Advanced preprocessing for air quality data with focus on PM2.5 prediction\n",
    "#     \"\"\"\n",
    "#     # Load and initialize data\n",
    "#     df = pd.read_csv(file_path, low_memory=False)  # Added low_memory=False to resolve mixed types warning\n",
    "#     df.drop(columns=['StationId'], inplace=True)\n",
    "# #     df = df[:num_samples]\n",
    "    \n",
    "#     # Handle missing values\n",
    "#     numeric_columns = df.select_dtypes(include=['float64']).columns\n",
    "#     imputer = SimpleImputer(strategy='mean')\n",
    "#     df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
    "    \n",
    "#     # Process datetime features\n",
    "#     df['Datetime'] = pd.to_datetime(df['Datetime'], format='mixed', errors='coerce')\n",
    "\n",
    "    \n",
    "#     # Extract time-based features\n",
    "#     df['Year'] = df['Datetime'].dt.year\n",
    "#     df['Month'] = df['Datetime'].dt.month\n",
    "#     df['Day'] = df['Datetime'].dt.day\n",
    "#     df['Hour'] = df['Datetime'].dt.hour\n",
    "#     df['DayOfWeek'] = df['Datetime'].dt.dayofweek\n",
    "#     df['IsWeekend'] = (df['Datetime'].dt.dayofweek >= 5).astype(int)\n",
    "    \n",
    "#     # Add seasonal features\n",
    "#     df['Season'] = df['Month'].map(lambda x: 1 if x in [12,1,2] else 2 if x in [3,4,5] else 3 if x in [6,7,8] else 4)\n",
    "    \n",
    "#     # Calculate rolling averages for PM2.5\n",
    "#     df['PM2.5_Rolling_Mean_3h'] = df['PM2.5'].rolling(window=3, min_periods=1).mean()\n",
    "#     df['PM2.5_Rolling_Mean_24h'] = df['PM2.5'].rolling(window=24, min_periods=1).mean()\n",
    "    \n",
    "#     # Create PM2.5 categories based on WHO guidelines\n",
    "#     bins = [0, 12, 35.4, 55.4, 150.4, 250.4, float('inf')]\n",
    "#     labels = ['Good', 'Moderate', 'Unhealthy for Sensitive', 'Unhealthy', 'Very Unhealthy', 'Hazardous']\n",
    "#     df['PM2.5_Category'] = pd.cut(df['PM2.5'], bins=bins, labels=labels)\n",
    "    \n",
    "#     # Drop datetime column\n",
    "#     df.drop(columns=['Datetime'], inplace=True)\n",
    "    \n",
    "#     # Balance classes using stratified sampling with modified groupby\n",
    "#     min_class_size = df['PM2.5_Category'].value_counts().min()\n",
    "    \n",
    "#     # Modified groupby to resolve deprecation warnings\n",
    "#     balanced_dfs = []\n",
    "#     for category in df['PM2.5_Category'].unique():\n",
    "#         category_df = df[df['PM2.5_Category'] == category]\n",
    "#         if len(category_df) > min_class_size:\n",
    "#             balanced_dfs.append(category_df.sample(min_class_size))\n",
    "#         else:\n",
    "#             balanced_dfs.append(category_df)\n",
    "    \n",
    "#     df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "#     # Encode categorical variables\n",
    "#     label_encoder = LabelEncoder()\n",
    "#     df_balanced['PM2.5_Category_Encoded'] = label_encoder.fit_transform(df_balanced['PM2.5_Category'])\n",
    "    \n",
    "#     # Scale features\n",
    "#     scaler = MinMaxScaler()\n",
    "#     columns_to_scale = df_balanced.select_dtypes(include=['float64', 'int64']).columns\n",
    "#     df_balanced_scaled = df_balanced.copy()\n",
    "#     df_balanced_scaled[columns_to_scale] = scaler.fit_transform(df_balanced[columns_to_scale])\n",
    "    \n",
    "#     # Prepare sequences for LSTM\n",
    "#     def create_sequences(data, seq_length):\n",
    "#         sequences = []\n",
    "#         targets = []\n",
    "        \n",
    "#         for i in range(len(data) - seq_length):\n",
    "#             # Get sequence of features\n",
    "#             sequence = data.iloc[i:(i + seq_length)]\n",
    "            \n",
    "#             # Get target (PM2.5 value at the next timestep)\n",
    "#             target = data.iloc[i + seq_length]['PM2.5']\n",
    "            \n",
    "#             sequences.append(sequence.values)\n",
    "#             targets.append(target)\n",
    "            \n",
    "#         return np.array(sequences), np.array(targets)\n",
    "    \n",
    "#     # Create sequences and targets\n",
    "#     X, y = create_sequences(df_balanced_scaled, sequence_length)\n",
    "    \n",
    "#     # Split into training and testing sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, test_size=0.2, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Create metadata dictionary\n",
    "#     metadata = {\n",
    "#         'feature_names': df_balanced_scaled.columns.tolist(),\n",
    "#         'pm25_categories': list(zip(label_encoder.classes_, range(len(label_encoder.classes_)))),\n",
    "#         'scaler': scaler,\n",
    "#         'label_encoder': label_encoder\n",
    "#     }\n",
    "    \n",
    "#     return {\n",
    "#         'X_train': X_train,\n",
    "#         'X_test': X_test,\n",
    "#         'y_train': y_train,\n",
    "#         'y_test': y_test,\n",
    "#         'metadata': metadata\n",
    "#     }\n",
    "\n",
    "# try:\n",
    "#     results1 = preprocess_air_quality_data(\n",
    "#         file_path='/kaggle/input/new-data/new_data.csv',\n",
    "#         num_samples=1048570,\n",
    "#         sequence_length=100\n",
    "#     )\n",
    "    \n",
    "#     results2 = preprocess_air_quality_data(\n",
    "#         file_path='/kaggle/input/all-other-set/second_set.csv',\n",
    "#         num_samples=1048570,\n",
    "#         sequence_length=100\n",
    "#     )\n",
    "    \n",
    "#     results3 = preprocess_air_quality_data(\n",
    "#         file_path='/kaggle/input/all-other-set/third_set.csv',\n",
    "#         num_samples=500000,\n",
    "#         sequence_length=100\n",
    "#     )\n",
    "    \n",
    "#     # Combine results using numpy.concatenate\n",
    "#     results = {\n",
    "#         'X_train': np.concatenate([results1['X_train'], results2['X_train'], results3['X_train']], axis=0),\n",
    "#         'X_test': np.concatenate([results1['X_test'], results2['X_test'], results3['X_test']], axis=0),\n",
    "#         'y_train': np.concatenate([results1['y_train'], results2['y_train'], results3['y_train']], axis=0),\n",
    "#         'y_test': np.concatenate([results1['y_test'], results2['y_test'], results3['y_test']], axis=0),\n",
    "#         'metadata': results1['metadata']  # Metadata can remain the same as they are shared across runs\n",
    "#     }\n",
    "    \n",
    "#     print(\"Training data shape:\", results['X_train'].shape)\n",
    "#     print(\"Testing data shape:\", results['X_test'].shape)\n",
    "#     print(\"Training targets shape:\", results['y_train'].shape)\n",
    "#     print(\"Testing targets shape:\", results['y_test'].shape)\n",
    "#     print(\"\\nFeature names:\", results['metadata']['feature_names'])\n",
    "#     print(\"\\nPM2.5 categories:\", results['metadata']['pm25_categories'])\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6063225,
     "sourceId": 9876032,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6104475,
     "sourceId": 9930950,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
